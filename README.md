# receipt_scanner
A repo for my combined project ideas from the Deep Learning and Learning from Images courses to reduce the stress of billing the shared apartment cash.

## Initial Idea
I wanted to simplify the process of bookkeeping in my shared flat. When me or my roommate do grocery shopping we keep the bills and after some time we manually go through them and check for who paid what. So we end up with three categories (Flatmate A, Flatmate B & Common amount). The procedure up to now was that I type in each amount manually into excel to calculate the final amount for each category to see who gets money back.  
My idea was to use a combination of color detection and OCR to reduce the hassle of manually typing in each amount and instead automatically scan and calculate the amounts. (The categorization would still have to be done manually because there is no recurring pattern behind what cash value on the receipt belongs to what category.)  

For this idea I wanted to programm an App that uses the videostream of a webcam to detect color coded cash values, scan the region of interests (ROIs) and predicts the shown float value with the help of an OCR model. After validating the predictions shown besides the live video stream one would confirm the scan and save the values e.g. to .csv. When the final cash receipt is scanned, predictions are confirmed and the results saved, a simple (e.g. sum) operation would return the final amount of each category.  
Initially I wanted use an of the shelf model from huggingface, kaggle or similar sites, but after some experiments I figured out that there was no really functional model available for my approach. Additionally I wanted to gain some hands on experience on training an OCR-model by myself with the help of my self-generated labeled image data. Due to the ongoing cybersecurity issue at my university the cluster is still out of service and I wasn't able to persue this approach any further for the time being.

## Current Approach
That's the reason why I came up with an alternate approach for the moment. I'm still using the app to detect color coded region of interests (ROIs) but instead of predicting the cash amounts during the live video stream (due to the lack of an of the shelf or self trained model), I will instead store the [unlabeled image snippets]() ad .png and afterwards label than with the help of Google's Vision AI. This model is free to use for < 1000 API requests per month. Because of this restriction I added a [preprocessing step]() to concatenate the image snippets to one single big canvas to reduce the API calls from a couple hundrets to one.  
Afterwards I store the [labeled image snippets]() to be able to manually validate it's accuracy. For my first test run of XXX image snippets the Google Vision AI failed only in [two cases]() from which I would dismiss [one]() as negligible because of it's (very!) poor image quality. The other one provides material for further [discussion]().  
After validation I simply read in all the filenames, convert them back to float values and calculate the final amounts. Even though I would like to avoid the steps of manually validating big amounts of labeled image data in my final approach through implementing a live prediction during the live video stream, I have to say that I'm positively surprised on how easy and simple the scanning, detecting and storing procedure is and on how exceptionally well the Google Vision AI model performes for a basically free to use approach.

## Future Plans
As mentioned previously I would like to implement a live prediction with a (self trained) local model and I want to improve the GUI further. Adding a slider for the color detection mode to calibrate the detected color spectrum should be done with ease for example. Furthermore I already did some tests with detecting/extracting the according items that belong to each cash value (e.g. Tomatoes, Milk, etc.) but because I lost some progress due to the Hack and because I have no interests in these information at the moment, I discarded the idea for now.

## Challenges
Besides a lot of challenges I've already overcome during the process of this project there is one major issue I currently just prevent by manual debugging and that's for the moment only tolerable because the Vision AI model performs with close to 100% success rate. When concatenating the image snippets to one big canvas to reduce API calls I surrender the explicit allocation of image to label. For the time being I just validate at the end if the Google prediction returns as many labels after [postprocessing]() as the orginial number of image snippets and if so, I do another validation by manually checking each labeled image snippet. If the first assertion ever fails (what happens close to 0% of the time) I have to manually check what image snippet caused the failed prediction, exclude it from the process and redo the prediction. This could be prevented by not plotting all image snippets on a large canvas, i.e. predicting each snippet individually and thus finding the problematic snippets faster, but this would very quickly go beyond the restriction of < 1000 API calls/month. But because of the very low error rate AND because the whole thing with the Google vision AI is only a transitional solution, I refrain from a more in-depth search for a solution.  
Another big challenge would be the if it's even feasible to train a model on those labeled image data. As already mentioned in my project presentation it's maybe neccessary to simplify the approach by iterating through each digit of the float value and recreating the whole value in the end, but due to the setback caused by the hack I'll have to look into that at a later time.
